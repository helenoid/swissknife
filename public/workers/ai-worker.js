/**
 * ü§ñ AI Inference Worker - Distributed AI Processing for Collaborative Computing
 * 
 * Features:
 * - Distributed AI model inference across peer networks
 * - GPU acceleration via WebGPU when available
 * - Model sharing and collaborative training
 * - Background AI processing for various applications
 * - Optimized tensor operations and neural network execution
 */

let workerId = '';
let webgpuDevice = null;
let modelCache = new Map();
let inferenceQueue = [];
let capabilities = {
  hasWebGPU: false,
  hasWebGL: false,
  maxTextureSize: 0,
  maxComputeWorkgroupSize: 0,
  supportedFormats: []
};

/**
 * Initialize AI worker with GPU capabilities
 */
async function initialize(config) {
  workerId = config.workerId;
  
  console.log(`ü§ñ AI worker ${workerId} initializing...`);
  
  // Detect WebGPU capabilities
  if ('gpu' in navigator) {
    try {
      const adapter = await navigator.gpu.requestAdapter();
      if (adapter) {
        webgpuDevice = await adapter.requestDevice();
        capabilities.hasWebGPU = true;
        capabilities.maxComputeWorkgroupSize = adapter.limits?.maxComputeWorkgroupSizeX || 256;
        console.log(`‚úÖ WebGPU initialized for AI worker ${workerId}`);
      }
    } catch (error) {
      console.warn('WebGPU not available:', error);
    }
  }
  
  // Detect WebGL capabilities as fallback
  const canvas = new OffscreenCanvas(1, 1);
  const gl = canvas.getContext('webgl2') || canvas.getContext('webgl');
  if (gl) {
    capabilities.hasWebGL = true;
    capabilities.maxTextureSize = gl.getParameter(gl.MAX_TEXTURE_SIZE);
    console.log(`‚úÖ WebGL available for AI worker ${workerId}`);
  }
  
  return {
    capabilities,
    workerId
  };
}

/**
 * Process AI inference tasks
 */
async function processTask(taskType, data) {
  switch (taskType) {
    case 'model-inference':
      return performModelInference(data);
    
    case 'tensor-operation':
      return performTensorOperation(data);
    
    case 'neural-network':
      return runNeuralNetwork(data);
    
    case 'text-generation':
      return generateText(data);
    
    case 'image-processing':
      return processImage(data);
    
    case 'collaborative-training':
      return collaborativeTraining(data);
    
    case 'model-optimization':
      return optimizeModel(data);
    
    case 'gpu-compute':
      return performGPUCompute(data);
    
    default:
      throw new Error(`Unknown AI task type: ${taskType}`);
  }
}

/**
 * Perform model inference
 */
async function performModelInference(data) {
  const { modelId, input, options = {} } = data;
  
  // Check if model is cached
  let model = modelCache.get(modelId);
  
  if (!model) {
    // Load model (simplified - in practice would load from IPFS or peer)
    model = await loadModel(modelId, options);
    modelCache.set(modelId, model);
  }
  
  console.log(`üß† Running inference for model ${modelId}`);
  
  // Perform inference based on model type
  const startTime = performance.now();
  
  let result;
  switch (model.type) {
    case 'neural-network':
      result = await runNeuralNetworkInference(model, input);
      break;
    
    case 'transformer':
      result = await runTransformerInference(model, input);
      break;
    
    case 'cnn':
      result = await runCNNInference(model, input);
      break;
    
    case 'rnn':
      result = await runRNNInference(model, input);
      break;
    
    default:
      throw new Error(`Unsupported model type: ${model.type}`);
  }
  
  const inferenceTime = performance.now() - startTime;
  
  return {
    result,
    modelId,
    inferenceTime,
    usedGPU: capabilities.hasWebGPU,
    timestamp: Date.now()
  };
}

/**
 * Load AI model (simplified implementation)
 */
async function loadModel(modelId, options) {
  console.log(`üì• Loading model ${modelId}...`);
  
  // Simulate model loading
  await new Promise(resolve => setTimeout(resolve, 100));
  
  // Return simplified model structure
  return {
    id: modelId,
    type: options.modelType || 'neural-network',
    layers: options.layers || [
      { type: 'dense', units: 128, activation: 'relu' },
      { type: 'dense', units: 64, activation: 'relu' },
      { type: 'dense', units: 10, activation: 'softmax' }
    ],
    weights: generateRandomWeights(options.layers || [128, 64, 10]),
    inputShape: options.inputShape || [784],
    outputShape: options.outputShape || [10]
  };
}

/**
 * Generate random weights for model
 */
function generateRandomWeights(layers) {
  const weights = [];
  
  for (let i = 0; i < layers.length - 1; i++) {
    const currentSize = layers[i];
    const nextSize = layers[i + 1];
    
    // Generate weight matrix
    const weightMatrix = [];
    for (let j = 0; j < currentSize; j++) {
      const row = [];
      for (let k = 0; k < nextSize; k++) {
        row.push((Math.random() - 0.5) * 2 * Math.sqrt(2 / currentSize));
      }
      weightMatrix.push(row);
    }
    
    // Generate bias vector
    const biases = [];
    for (let j = 0; j < nextSize; j++) {
      biases.push((Math.random() - 0.5) * 0.1);
    }
    
    weights.push({ weights: weightMatrix, biases });
  }
  
  return weights;
}

/**
 * Run neural network inference
 */
async function runNeuralNetworkInference(model, input) {
  let output = Array.isArray(input) ? [...input] : [input];
  
  // Forward pass through layers
  for (let layerIndex = 0; layerIndex < model.weights.length; layerIndex++) {
    const layer = model.weights[layerIndex];
    const newOutput = [];
    
    // Matrix multiplication
    for (let j = 0; j < layer.weights[0].length; j++) {
      let sum = layer.biases[j];
      
      for (let i = 0; i < output.length; i++) {
        sum += output[i] * layer.weights[i][j];
      }
      
      // Apply activation function (ReLU for hidden layers, softmax for output)
      if (layerIndex < model.weights.length - 1) {
        newOutput.push(Math.max(0, sum)); // ReLU
      } else {
        newOutput.push(sum); // Pre-softmax
      }
    }
    
    // Apply softmax to final layer
    if (layerIndex === model.weights.length - 1) {
      const maxVal = Math.max(...newOutput);
      const expValues = newOutput.map(x => Math.exp(x - maxVal));
      const sumExp = expValues.reduce((a, b) => a + b, 0);
      output = expValues.map(x => x / sumExp);
    } else {
      output = newOutput;
    }
  }
  
  return {
    predictions: output,
    confidence: Math.max(...output),
    predictedClass: output.indexOf(Math.max(...output))
  };
}

/**
 * Perform tensor operations
 */
async function performTensorOperation(data) {
  const { operation, tensors, options = {} } = data;
  
  console.log(`üî¢ Performing tensor operation: ${operation}`);
  
  switch (operation) {
    case 'matmul':
      return matrixMultiply(tensors.a, tensors.b);
    
    case 'add':
      return tensorAdd(tensors.a, tensors.b);
    
    case 'subtract':
      return tensorSubtract(tensors.a, tensors.b);
    
    case 'conv2d':
      return convolution2D(tensors.input, tensors.kernel, options);
    
    case 'pooling':
      return maxPooling(tensors.input, options);
    
    case 'normalization':
      return batchNormalization(tensors.input, options);
    
    default:
      throw new Error(`Unknown tensor operation: ${operation}`);
  }
}

/**
 * Matrix multiplication
 */
function matrixMultiply(a, b) {
  if (!Array.isArray(a) || !Array.isArray(b)) {
    throw new Error('Invalid matrix format');
  }
  
  const rows = a.length;
  const cols = b[0].length;
  const shared = a[0].length;
  
  if (shared !== b.length) {
    throw new Error('Matrix dimensions do not match for multiplication');
  }
  
  const result = [];
  
  for (let i = 0; i < rows; i++) {
    result[i] = [];
    for (let j = 0; j < cols; j++) {
      let sum = 0;
      for (let k = 0; k < shared; k++) {
        sum += a[i][k] * b[k][j];
      }
      result[i][j] = sum;
    }
  }
  
  return result;
}

/**
 * Tensor addition
 */
function tensorAdd(a, b) {
  if (a.length !== b.length) {
    throw new Error('Tensor dimensions do not match');
  }
  
  return a.map((val, i) => 
    Array.isArray(val) ? tensorAdd(val, b[i]) : val + b[i]
  );
}

/**
 * Tensor subtraction
 */
function tensorSubtract(a, b) {
  if (a.length !== b.length) {
    throw new Error('Tensor dimensions do not match');
  }
  
  return a.map((val, i) => 
    Array.isArray(val) ? tensorSubtract(val, b[i]) : val - b[i]
  );
}

/**
 * 2D Convolution
 */
function convolution2D(input, kernel, options = {}) {
  const { stride = 1, padding = 0 } = options;
  
  const inputHeight = input.length;
  const inputWidth = input[0].length;
  const kernelHeight = kernel.length;
  const kernelWidth = kernel[0].length;
  
  const outputHeight = Math.floor((inputHeight + 2 * padding - kernelHeight) / stride + 1);
  const outputWidth = Math.floor((inputWidth + 2 * padding - kernelWidth) / stride + 1);
  
  const result = [];
  
  for (let i = 0; i < outputHeight; i++) {
    result[i] = [];
    for (let j = 0; j < outputWidth; j++) {
      let sum = 0;
      
      for (let ki = 0; ki < kernelHeight; ki++) {
        for (let kj = 0; kj < kernelWidth; kj++) {
          const inputRow = i * stride + ki - padding;
          const inputCol = j * stride + kj - padding;
          
          if (inputRow >= 0 && inputRow < inputHeight && 
              inputCol >= 0 && inputCol < inputWidth) {
            sum += input[inputRow][inputCol] * kernel[ki][kj];
          }
        }
      }
      
      result[i][j] = sum;
    }
  }
  
  return result;
}

/**
 * Max pooling operation
 */
function maxPooling(input, options = {}) {
  const { poolSize = 2, stride = 2 } = options;
  
  const inputHeight = input.length;
  const inputWidth = input[0].length;
  
  const outputHeight = Math.floor((inputHeight - poolSize) / stride + 1);
  const outputWidth = Math.floor((inputWidth - poolSize) / stride + 1);
  
  const result = [];
  
  for (let i = 0; i < outputHeight; i++) {
    result[i] = [];
    for (let j = 0; j < outputWidth; j++) {
      let maxVal = -Infinity;
      
      for (let pi = 0; pi < poolSize; pi++) {
        for (let pj = 0; pj < poolSize; pj++) {
          const inputRow = i * stride + pi;
          const inputCol = j * stride + pj;
          
          if (inputRow < inputHeight && inputCol < inputWidth) {
            maxVal = Math.max(maxVal, input[inputRow][inputCol]);
          }
        }
      }
      
      result[i][j] = maxVal;
    }
  }
  
  return result;
}

/**
 * Perform GPU compute operations using WebGPU
 */
async function performGPUCompute(data) {
  if (!capabilities.hasWebGPU || !webgpuDevice) {
    throw new Error('WebGPU not available for GPU compute');
  }
  
  const { shader, buffers, workgroupSize = [64, 1, 1] } = data;
  
  console.log(`üöÄ Running GPU compute task`);
  
  try {
    // Create compute shader
    const computeShader = webgpuDevice.createShaderModule({
      code: shader
    });
    
    // Create compute pipeline
    const computePipeline = webgpuDevice.createComputePipeline({
      layout: 'auto',
      compute: {
        module: computeShader,
        entryPoint: 'main'
      }
    });
    
    // Create buffers
    const gpuBuffers = [];
    const stagingBuffers = [];
    
    for (const buffer of buffers) {
      const gpuBuffer = webgpuDevice.createBuffer({
        size: buffer.data.byteLength,
        usage: GPUBufferUsage.STORAGE | GPUBufferUsage.COPY_DST | GPUBufferUsage.COPY_SRC
      });
      
      const stagingBuffer = webgpuDevice.createBuffer({
        size: buffer.data.byteLength,
        usage: GPUBufferUsage.MAP_READ | GPUBufferUsage.COPY_DST
      });
      
      // Write data to GPU buffer
      webgpuDevice.queue.writeBuffer(gpuBuffer, 0, buffer.data);
      
      gpuBuffers.push(gpuBuffer);
      stagingBuffers.push(stagingBuffer);
    }
    
    // Create bind group
    const bindGroup = webgpuDevice.createBindGroup({
      layout: computePipeline.getBindGroupLayout(0),
      entries: gpuBuffers.map((buffer, index) => ({
        binding: index,
        resource: { buffer }
      }))
    });
    
    // Create command encoder
    const commandEncoder = webgpuDevice.createCommandEncoder();
    
    // Dispatch compute
    const passEncoder = commandEncoder.beginComputePass();
    passEncoder.setPipeline(computePipeline);
    passEncoder.setBindGroup(0, bindGroup);
    passEncoder.dispatchWorkgroups(
      Math.ceil(buffers[0].data.byteLength / 4 / workgroupSize[0]),
      workgroupSize[1],
      workgroupSize[2]
    );
    passEncoder.end();
    
    // Copy results to staging buffers
    for (let i = 0; i < gpuBuffers.length; i++) {
      commandEncoder.copyBufferToBuffer(
        gpuBuffers[i], 0,
        stagingBuffers[i], 0,
        buffers[i].data.byteLength
      );
    }
    
    // Submit commands
    webgpuDevice.queue.submit([commandEncoder.finish()]);
    
    // Read results
    const results = [];
    for (let i = 0; i < stagingBuffers.length; i++) {
      await stagingBuffers[i].mapAsync(GPUMapMode.READ);
      const arrayBuffer = stagingBuffers[i].getMappedRange();
      results.push(new Float32Array(arrayBuffer.slice()));
      stagingBuffers[i].unmap();
    }
    
    return {
      results,
      computeTime: performance.now(),
      usedGPU: true
    };
    
  } catch (error) {
    console.error('GPU compute error:', error);
    throw error;
  }
}

/**
 * Generate text using AI models
 */
async function generateText(data) {
  const { prompt, maxTokens = 100, temperature = 0.7, model = 'gpt-simple' } = data;
  
  console.log(`üìù Generating text for prompt: "${prompt.substring(0, 50)}..."`);
  
  // Simplified text generation (in practice would use a proper language model)
  const vocab = [
    'the', 'and', 'to', 'of', 'a', 'in', 'is', 'it', 'you', 'that', 'he', 'was', 'for', 'on',
    'are', 'as', 'with', 'his', 'they', 'I', 'at', 'be', 'this', 'have', 'from', 'or', 'one',
    'had', 'by', 'word', 'but', 'not', 'what', 'all', 'were', 'we', 'when', 'your', 'can',
    'said', 'there', 'each', 'which', 'she', 'do', 'how', 'their', 'if', 'will', 'up', 'other',
    'about', 'out', 'many', 'then', 'them', 'these', 'so', 'some', 'her', 'would', 'make',
    'like', 'into', 'him', 'time', 'has', 'two', 'more', 'go', 'no', 'way', 'could', 'my'
  ];
  
  let generatedText = prompt;
  
  for (let i = 0; i < maxTokens; i++) {
    // Simple next token prediction (replace with proper model)
    const lastWords = generatedText.split(' ').slice(-3);
    let nextWord;
    
    if (Math.random() < temperature) {
      // Random selection for creativity
      nextWord = vocab[Math.floor(Math.random() * vocab.length)];
    } else {
      // Deterministic selection based on context
      const contextHash = lastWords.join('').split('').reduce((a, b) => {
        a = ((a << 5) - a) + b.charCodeAt(0);
        return a & a;
      }, 0);
      nextWord = vocab[Math.abs(contextHash) % vocab.length];
    }
    
    generatedText += ' ' + nextWord;
    
    // Stop on sentence end
    if (nextWord.endsWith('.') || nextWord.endsWith('!') || nextWord.endsWith('?')) {
      break;
    }
  }
  
  return {
    generatedText,
    tokenCount: generatedText.split(' ').length - prompt.split(' ').length,
    model,
    temperature
  };
}

/**
 * Process images using AI
 */
async function processImage(data) {
  const { imageData, operation, options = {} } = data;
  
  console.log(`üñºÔ∏è Processing image with operation: ${operation}`);
  
  switch (operation) {
    case 'edge-detection':
      return edgeDetection(imageData, options);
    
    case 'blur':
      return gaussianBlur(imageData, options);
    
    case 'classification':
      return classifyImage(imageData, options);
    
    case 'style-transfer':
      return styleTransfer(imageData, options);
    
    default:
      throw new Error(`Unknown image operation: ${operation}`);
  }
}

/**
 * Simple edge detection
 */
function edgeDetection(imageData, options = {}) {
  const { threshold = 50 } = options;
  const { data, width, height } = imageData;
  const result = new Uint8ClampedArray(data.length);
  
  // Sobel edge detection
  const sobelX = [[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]];
  const sobelY = [[-1, -2, -1], [0, 0, 0], [1, 2, 1]];
  
  for (let y = 1; y < height - 1; y++) {
    for (let x = 1; x < width - 1; x++) {
      let gx = 0, gy = 0;
      
      for (let ky = -1; ky <= 1; ky++) {
        for (let kx = -1; kx <= 1; kx++) {
          const pixel = ((y + ky) * width + (x + kx)) * 4;
          const gray = (data[pixel] + data[pixel + 1] + data[pixel + 2]) / 3;
          
          gx += gray * sobelX[ky + 1][kx + 1];
          gy += gray * sobelY[ky + 1][kx + 1];
        }
      }
      
      const magnitude = Math.sqrt(gx * gx + gy * gy);
      const edge = magnitude > threshold ? 255 : 0;
      
      const index = (y * width + x) * 4;
      result[index] = edge;     // R
      result[index + 1] = edge; // G
      result[index + 2] = edge; // B
      result[index + 3] = 255;  // A
    }
  }
  
  return {
    imageData: {
      data: result,
      width,
      height
    },
    operation: 'edge-detection',
    parameters: { threshold }
  };
}

/**
 * Collaborative training coordination
 */
async function collaborativeTraining(data) {
  const { modelId, trainingData, peers, epoch } = data;
  
  console.log(`ü§ù Collaborative training for model ${modelId}, epoch ${epoch}`);
  
  // Simulate federated learning step
  const localGradients = computeLocalGradients(trainingData);
  
  return {
    modelId,
    epoch,
    localGradients,
    trainingLoss: Math.random() * 0.5 + 0.1, // Simulated loss
    samplesProcessed: trainingData.length,
    peerId: workerId
  };
}

/**
 * Compute local gradients (simplified)
 */
function computeLocalGradients(trainingData) {
  // Simplified gradient computation
  const gradients = [];
  
  for (let i = 0; i < trainingData.length; i++) {
    const sample = trainingData[i];
    // Simulate gradient calculation
    gradients.push({
      layer: Math.floor(Math.random() * 3),
      weights: Array.from({ length: 10 }, () => (Math.random() - 0.5) * 0.01),
      bias: (Math.random() - 0.5) * 0.01
    });
  }
  
  return gradients;
}

// Worker message handling
self.onmessage = async function(event) {
  const { type, taskId, data } = event.data;
  
  try {
    let result;
    
    switch (type) {
      case 'init':
        result = await initialize(data);
        break;
      
      case 'task':
        result = await processTask(data.taskType, data.data);
        break;
      
      default:
        throw new Error(`Unknown message type: ${type}`);
    }
    
    self.postMessage({
      type: 'task-completed',
      taskId,
      result
    });
    
  } catch (error) {
    console.error('AI worker error:', error);
    
    self.postMessage({
      type: 'task-completed',
      taskId,
      error: error.message
    });
  }
};